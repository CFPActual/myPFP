{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What drives the price of a car?\n",
    "\n",
    "![](images/kurt.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Understanding\n",
    "\n",
    "After considering the business understanding, we want to get familiar with our data.  Write down some steps that you would take to get to know the dataset and identify any quality issues within.  Take time to get to know the dataset and explore what information it contains and how this could be used to inform your business understanding."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-16T16:40:22.777902Z",
     "start_time": "2025-09-16T16:40:22.623543Z"
    }
   },
   "source": [
    "from itertools import product\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import RidgeCV\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error, mean_absolute_percentage_error\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from xgboost import XGBRegressor\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns"
   ],
   "outputs": [],
   "execution_count": 92
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-16T16:40:29.058568Z",
     "start_time": "2025-09-16T16:40:28.274349Z"
    }
   },
   "source": [
    "df = pd.read_csv(\"data/vehicles.csv\")\n",
    "df.head()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "           id                  region  price  year manufacturer model  \\\n",
       "0  7222695916                prescott   6000   NaN          NaN   NaN   \n",
       "1  7218891961            fayetteville  11900   NaN          NaN   NaN   \n",
       "2  7221797935            florida keys  21000   NaN          NaN   NaN   \n",
       "3  7222270760  worcester / central MA   1500   NaN          NaN   NaN   \n",
       "4  7210384030              greensboro   4900   NaN          NaN   NaN   \n",
       "\n",
       "  condition cylinders fuel  odometer title_status transmission  VIN drive  \\\n",
       "0       NaN       NaN  NaN       NaN          NaN          NaN  NaN   NaN   \n",
       "1       NaN       NaN  NaN       NaN          NaN          NaN  NaN   NaN   \n",
       "2       NaN       NaN  NaN       NaN          NaN          NaN  NaN   NaN   \n",
       "3       NaN       NaN  NaN       NaN          NaN          NaN  NaN   NaN   \n",
       "4       NaN       NaN  NaN       NaN          NaN          NaN  NaN   NaN   \n",
       "\n",
       "  size type paint_color state  \n",
       "0  NaN  NaN         NaN    az  \n",
       "1  NaN  NaN         NaN    ar  \n",
       "2  NaN  NaN         NaN    fl  \n",
       "3  NaN  NaN         NaN    ma  \n",
       "4  NaN  NaN         NaN    nc  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>region</th>\n",
       "      <th>price</th>\n",
       "      <th>year</th>\n",
       "      <th>manufacturer</th>\n",
       "      <th>model</th>\n",
       "      <th>condition</th>\n",
       "      <th>cylinders</th>\n",
       "      <th>fuel</th>\n",
       "      <th>odometer</th>\n",
       "      <th>title_status</th>\n",
       "      <th>transmission</th>\n",
       "      <th>VIN</th>\n",
       "      <th>drive</th>\n",
       "      <th>size</th>\n",
       "      <th>type</th>\n",
       "      <th>paint_color</th>\n",
       "      <th>state</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7222695916</td>\n",
       "      <td>prescott</td>\n",
       "      <td>6000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>az</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7218891961</td>\n",
       "      <td>fayetteville</td>\n",
       "      <td>11900</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7221797935</td>\n",
       "      <td>florida keys</td>\n",
       "      <td>21000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>fl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7222270760</td>\n",
       "      <td>worcester / central MA</td>\n",
       "      <td>1500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ma</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7210384030</td>\n",
       "      <td>greensboro</td>\n",
       "      <td>4900</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nc</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 93
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-16T16:40:42.755697Z",
     "start_time": "2025-09-16T16:40:42.739594Z"
    }
   },
   "cell_type": "code",
   "source": "test_results = {\"MAE\": [], \"R\":[]}",
   "outputs": [],
   "execution_count": 94
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-16T17:42:35.146368Z",
     "start_time": "2025-09-16T17:42:35.111256Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def run_drill(nan_thresh=5, price_thresh=5e5, mileage_thresh=1e6, commonality_thresh=10, components=25):\n",
    "    df_lite = df.drop(columns=[\"id\", \"size\", \"paint_color\"])  # retaining VIN until after the VIN duplicate check\n",
    "\n",
    "    df_lite = df_lite[df_lite.notnull().sum(axis=1) >= nan_thresh] # dropping rows with fewer than n non-NaN values\n",
    "\n",
    "\n",
    "    # Drop VIN duplicates then drop the VIN column since it has served its purpose\n",
    "    df_lite = df_lite.drop_duplicates(subset='VIN', keep='first')\n",
    "    df_lite = df_lite.drop(columns=[\"VIN\"])\n",
    "\n",
    "\n",
    "    # Drop exact duplicate rows (keep first occurrence)\n",
    "    df_lite = df_lite.drop_duplicates()\n",
    "    df_clean_imp = df_lite.copy()\n",
    "    working_df = df_clean_imp.copy()\n",
    "\n",
    "    # Mean/Mode Fill the working df if that is what is needed\n",
    "    for col in working_df.columns:\n",
    "        if working_df[col].dtype in ['float64', 'int64']:\n",
    "            working_df[col] = working_df[col].fillna(working_df[col].mean())\n",
    "        elif working_df[col].dtype == 'object':\n",
    "            working_df[col] = working_df[col].fillna(working_df[col].mode()[0])\n",
    "\n",
    "    # Prices and Odometer values approaching 1M should be examined/handled\n",
    "    working_df = working_df[working_df['price'] <= price_thresh]\n",
    "    working_df = working_df[(working_df['odometer'] <= mileage_thresh)]\n",
    "    working_df.reset_index(drop=True, inplace=True)\n",
    "    working_df.drop(columns=[\"region\", \"model\"], inplace=True)\n",
    "\n",
    "    american = [\n",
    "        \"gmc\", \"chevrolet\", \"ford\", \"jeep\", \"ram\", \"cadillac\", \"buick\",\n",
    "        \"lincoln\", \"chrysler\", \"dodge\", \"pontiac\", \"mercury\", \"saturn\",\n",
    "        \"tesla\", \"harley-davidson\"]\n",
    "\n",
    "    japanese = [\n",
    "        \"toyota\", \"nissan\", \"mazda\", \"honda\", \"lexus\", \"acura\", \"subaru\",\n",
    "        \"mitsubishi\", \"infiniti\", \"datsun\"]\n",
    "\n",
    "    asian = [\n",
    "        \"hyundai\", \"kia\"]\n",
    "\n",
    "    european = [\n",
    "        \"volvo\", \"audi\", \"bmw\", \"mercedes-benz\", \"porsche\", \"fiat\",\n",
    "        \"jaguar\", \"mini\", \"rover\", \"land rover\", \"alfa-romeo\",\n",
    "        \"aston-martin\", \"ferrari\", \"volkswagen\"]\n",
    "\n",
    "    manufacturer_region = {}\n",
    "\n",
    "    for brand in american:\n",
    "        manufacturer_region[brand] = \"american\"\n",
    "    for brand in japanese:\n",
    "        manufacturer_region[brand] = \"japanese\"\n",
    "    for brand in asian:\n",
    "        manufacturer_region[brand] = \"asian\"\n",
    "    for brand in european:\n",
    "        manufacturer_region[brand] = \"european\"\n",
    "\n",
    "    working_df['manufacturer_region'] = working_df['manufacturer'].map(manufacturer_region)\n",
    "    working_df.drop(columns=['manufacturer'], inplace=True)\n",
    "\n",
    "    # Droping these 469 records wont hurt\n",
    "    working_df = working_df[working_df['cylinders'] != 'other']\n",
    "    working_df['cylinders'] = working_df['cylinders'].str.extract('(\\d+)').astype('int64')\n",
    "\n",
    "    # COLLAPSE THE TYPE FEATURE TO THE TOP N MOST COMMON VALUES\n",
    "    top_n = working_df['type'].value_counts().nlargest(commonality_thresh).index\n",
    "    working_df['type'] = working_df['type'].apply(lambda x: x if x in top_n else 'other')\n",
    "    working_df['type'] = working_df['type'].str.lower()\n",
    "\n",
    "    # GROUP STATES BY NATIONAL REGION\n",
    "    region = {\n",
    "        \"South\": [\"wv\", \"dc\", \"md\", \"va\",\n",
    "                  \"ky\", \"tn\", \"nc\", \"ms\",\n",
    "                  \"ar\", \"la\", \"al\", \"ga\", \"sc\",\n",
    "                  \"fl\", \"de\", \"pr\"],\n",
    "        \"Southwest\": [\"az\", \"nm\", \"ok\", \"tx\"],\n",
    "        \"West\": [\"wa\", \"or\", \"ca\", \"nv\", \"id\", \"mt\",\n",
    "                 \"wy\", \"ut\", \"co\", \"ak\", \"hi\"],\n",
    "        \"Midwest\": [\"nd\", \"sd\", \"ne\", \"ks\", \"mn\",\n",
    "                    \"ia\", \"mo\", \"wi\", \"il\", \"mi\", \"in\",\n",
    "                    \"oh\"],\n",
    "        \"Northeast\": [\"me\", \"vt\", \"ny\", \"nh\", \"ma\",\n",
    "                      \"ri\", \"ct\", \"nj\", \"pa\"]\n",
    "    }\n",
    "\n",
    "    def get_region(state):\n",
    "        if state in region[\"South\"]:\n",
    "            return \"south\"\n",
    "        elif state in region[\"Southwest\"]:\n",
    "            return \"southwest\"\n",
    "        elif state in region[\"West\"]:\n",
    "            return \"west\"\n",
    "        elif state in region[\"Midwest\"]:\n",
    "            return \"midwest\"\n",
    "        elif state in region[\"Northeast\"]:\n",
    "            return \"northeast\"\n",
    "        else:\n",
    "            return \"unknown\"\n",
    "\n",
    "    state_col = working_df[\"state\"].tolist()\n",
    "    region_col = []\n",
    "    for st in state_col:\n",
    "        region_col.append(get_region(st))\n",
    "    working_df[\"title_region\"] = region_col\n",
    "    working_df.drop(columns=[\"state\"], inplace=True)\n",
    "\n",
    "\n",
    "    # ----- Columns -----\n",
    "    ordinal_col = ['condition']\n",
    "    ordinal_order = [['salvage', 'fair', 'good', 'excellent', 'like new', 'new']]\n",
    "\n",
    "    nominal_cols = [\n",
    "        'fuel', 'title_status', 'transmission', 'drive',\n",
    "        'type', 'manufacturer_region', 'title_region'\n",
    "    ]\n",
    "\n",
    "    numeric_cols = ['price', 'year', 'odometer', 'cylinders']\n",
    "\n",
    "    # ----- Transformers -----\n",
    "    ordinal_enc = OrdinalEncoder(categories=ordinal_order)\n",
    "    nominal_enc = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n",
    "\n",
    "    preprocessor = ColumnTransformer([\n",
    "        ('ordinal', ordinal_enc, ordinal_col),\n",
    "        ('nominal', nominal_enc, nominal_cols)\n",
    "    ], remainder='passthrough')  # keep numeric features\n",
    "\n",
    "    # ----- Final pipeline -----\n",
    "    full_pipeline = Pipeline([\n",
    "        ('encode', preprocessor)\n",
    "    ])\n",
    "\n",
    "    # ----- Transform the dataset -----\n",
    "    working_df_transformed = full_pipeline.fit_transform(working_df)\n",
    "\n",
    "    # Get feature names\n",
    "    encoded_cols = list(full_pipeline.named_steps['encode'].transformers_[1][1].get_feature_names_out(nominal_cols))\n",
    "    final_columns = ordinal_col + encoded_cols + numeric_cols\n",
    "\n",
    "    working_df_encoded = pd.DataFrame(working_df_transformed, columns=final_columns)\n",
    "\n",
    "\n",
    "    # Step 1: isolate features and target\n",
    "    X = working_df_encoded.drop(\"price\", axis=1)\n",
    "    y = working_df_encoded[\"price\"]\n",
    "\n",
    "    # Step 2: split into train_temp (80%) and test (20%)\n",
    "    X_train_temp, X_test, y_train_temp, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42\n",
    "    )\n",
    "\n",
    "    # Step 3: split train_temp into train (75% of 80%) and dev (25% of 80%)\n",
    "    X_train, X_dev, y_train, y_dev = train_test_split(\n",
    "        X_train_temp, y_train_temp, test_size=0.25, random_state=42\n",
    "    )\n",
    "\n",
    "    numeric_features = ['year', 'odometer', 'cylinders']  # Possibly 'price' if you're transforming it\n",
    "\n",
    "\n",
    "    # Instantiate the scaler\n",
    "    scaler = StandardScaler()\n",
    "\n",
    "    # Fit on training data ONLY\n",
    "    scaler.fit(X_train[numeric_features])\n",
    "\n",
    "    # Transform all sets\n",
    "    X_train[numeric_features] = scaler.transform(X_train[numeric_features])\n",
    "    X_dev[numeric_features] = scaler.transform(X_dev[numeric_features])\n",
    "    X_test[numeric_features] = scaler.transform(X_test[numeric_features])\n",
    "\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_dev_scaled = scaler.transform(X_dev)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    pca = PCA(n_components=components, random_state=42)  # OR: n_components=20\n",
    "    X_train_pca = pca.fit_transform(X_train_scaled)\n",
    "    X_dev_pca = pca.transform(X_dev_scaled)\n",
    "    X_test_pca = pca.transform(X_test_scaled)\n",
    "\n",
    "\n",
    "    # Create transformer\n",
    "    poly = PolynomialFeatures(degree=2, include_bias=False)\n",
    "\n",
    "    # Fit + transform train only\n",
    "    X_train_poly = poly.fit_transform(X_train_scaled)\n",
    "    X_dev_poly = poly.transform(X_dev_scaled)\n",
    "    X_test_poly = poly.transform(X_test_scaled)\n",
    "\n",
    "    #Ridge Regressor\n",
    "    model = Ridge(alpha=100)\n",
    "    model.fit(X_train_poly, y_train)\n",
    "    y_pred = model.predict(X_dev_poly)\n",
    "    test_results[\"MAE\"].append(mean_absolute_error(y_dev, y_pred))\n",
    "    test_results[\"R\"].append(r2_score(y_dev, y_pred))\n",
    "\n",
    "\n",
    "    # RRandom Forest Regressor\n",
    "    # model = RandomForestRegressor(\n",
    "    #     n_estimators=100,\n",
    "    #     max_depth=None,\n",
    "    #     random_state=42,\n",
    "    #     n_jobs=-1\n",
    "    # )\n",
    "\n",
    "    # Train\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Predict on dev set\n",
    "    y_pred = model.predict(X_dev)\n",
    "\n",
    "    # Evaluate\n",
    "    test_results[\"MAE\"].append(mean_absolute_error(y_dev, y_pred))\n",
    "    test_results[\"R\"].append(r2_score(y_dev, y_pred))"
   ],
   "outputs": [],
   "execution_count": 122
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "FOR SINGLE RUN TESTING"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-16T17:42:36.612040Z",
     "start_time": "2025-09-16T17:42:36.599002Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# run_drill()\n",
    "# print(test_results)\n",
    "# print(f\"\\nTime elapsed: {(toc() - start)/60:0.2f} minutes.\")"
   ],
   "outputs": [],
   "execution_count": 123
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "THRESHOLD TEST BLOCK"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-16T17:42:13.924589Z",
     "start_time": "2025-09-16T17:42:06.940866Z"
    }
   },
   "cell_type": "code",
   "source": [
    "nan_threshes = [4]\n",
    "price_threshes = [1e6]\n",
    "mileage_threshes = [1e6]\n",
    "commonality_threshes = [9, 10]\n",
    "\n",
    "\n",
    "for nan in nan_threshes:\n",
    "    for price in price_threshes:\n",
    "        for mileage in mileage_threshes:\n",
    "            for commonality in commonality_threshes:\n",
    "                run_drill(nan_thresh=nan, price_thresh=price,mileage_thresh=mileage, commonality_thresh=commonality)\n",
    "print(test_results)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'MAE': [7392.684839807434, 8161.553335541447, 7400.264632099564, 8164.332614132231], 'R': [0.34799487629364256, 0.2754790066206867, 0.347219571943413, 0.27567113994902703]}\n"
     ]
    }
   ],
   "execution_count": 120
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "THRESH TEST RESULTS"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-16T17:42:13.988946Z",
     "start_time": "2025-09-16T17:42:13.972819Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Rebuild param grid\n",
    "param_grid = list(product(nan_threshes, price_threshes, mileage_threshes, commonality_threshes))\n",
    "param_df = pd.DataFrame(param_grid, columns=[\"nan_thresh\", \"price_thresh\", \"mileage_thresh\", \"commonality_thresh\"])\n",
    "\n",
    "# Rehydrate results\n",
    "results_df = pd.DataFrame(test_results)\n",
    "results_df = pd.concat([results_df, param_df], axis=1)\n",
    "\n",
    "# Find best row by MAE or RÂ²\n",
    "best_mae_idx = results_df[\"MAE\"].idxmin()\n",
    "best_r2_idx = results_df[\"R\"].idxmax()\n",
    "\n",
    "print(\"ðŸ”¹ Best by MAE:\")\n",
    "print(results_df.loc[best_mae_idx])\n",
    "\n",
    "print(\"\\nðŸ”¹ Best by RÂ²:\")\n",
    "print(results_df.loc[best_r2_idx])\n",
    "\n",
    "test_results[\"MAE\"].clear()\n",
    "test_results[\"R\"].clear()\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”¹ Best by MAE:\n",
      "MAE                      7392.684840\n",
      "R                           0.347995\n",
      "nan_thresh                  4.000000\n",
      "price_thresh          1000000.000000\n",
      "mileage_thresh        1000000.000000\n",
      "commonality_thresh          9.000000\n",
      "Name: 0, dtype: float64\n",
      "\n",
      "ðŸ”¹ Best by RÂ²:\n",
      "MAE                      7392.684840\n",
      "R                           0.347995\n",
      "nan_thresh                  4.000000\n",
      "price_thresh          1000000.000000\n",
      "mileage_thresh        1000000.000000\n",
      "commonality_thresh          9.000000\n",
      "Name: 0, dtype: float64\n"
     ]
    }
   ],
   "execution_count": 121
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
